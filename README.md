# ğŸš€ Code Repository RAG System

<div align="center">

![Code Repository RAG](https://img.shields.io/badge/Code%20Repository-RAG-blue?style=for-the-badge&logo=github)
![Python](https://img.shields.io/badge/Python-3.8+-green?style=for-the-badge&logo=python)
![LangChain](https://img.shields.io/badge/LangChain-0.1+-orange?style=for-the-badge&logo=chainlink)
![Azure OpenAI](https://img.shields.io/badge/Azure-OpenAI-purple?style=for-the-badge&logo=microsoft)

**An intelligent RAG system for code repository analysis with dual granularity processing**

[ğŸ“– Documentation](#-documentation) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ”§ Configuration](#-configuration) â€¢ [ğŸ“Š Performance](#-performance-analysis)

</div>

---

## âœ¨ Overview

Welcome to **Code Repository RAG System** - an advanced Retrieval-Augmented Generation system specifically designed for analyzing and understanding code repositories. Built with modern AI technologies, this system provides intelligent question-answering capabilities about codebases with support for both **file-level** and **chunk-level** processing granularities.

### ğŸ¯ Key Highlights

- ğŸ¨ **Dual Processing Modes**: Choose between file-level summaries or detailed chunk analysis
- ğŸ§  **Intelligent Caching**: Smart caching system for optimal performance
- ğŸ” **Multi-language Support**: Python, JavaScript, Java, C++, and more
- âš¡ **Azure OpenAI Integration**: Powered by GPT models via Azure
- ğŸ“ˆ **Performance Optimized**: Advanced caching and incremental processing
- ğŸ› ï¸ **Modular Architecture**: Easy to extend and customize

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Code Parser   â”‚â”€â”€â”€â–¶â”‚   Document      â”‚â”€â”€â”€â–¶â”‚   Vector Store   â”‚
â”‚   (Multi-lang)  â”‚    â”‚   Processing    â”‚    â”‚   (Chroma)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Granularity   â”‚    â”‚   Embedding     â”‚    â”‚   QA System     â”‚
â”‚   Selection     â”‚â”€â”€â”€â–¶â”‚   Generation    â”‚â”€â”€â”€â–¶â”‚   (Azure OpenAI)â”‚
â”‚ (File/Chunk)    â”‚    â”‚ (HuggingFace)   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
Code-Repository-RAG/
â”œâ”€â”€ ğŸ“„ main.py                 # ğŸš€ Main entry point
â”œâ”€â”€ ğŸ“„ requirements.txt        # ğŸ“¦ Python dependencies
â”œâ”€â”€ ğŸ“„ .env                    # âš™ï¸ Environment configuration
â”œâ”€â”€ ğŸ“„ README.md              # ğŸ“– This documentation
â”‚
â”œâ”€â”€ ğŸ“ processed_repos/       # ğŸ’¾ Cache & vector stores
â”‚   â””â”€â”€ ğŸ“ {repo_hash}/       # Repository-specific data
â”‚       â”œâ”€â”€ ğŸ“ chunk/         # Chunk granularity data
â”‚       â””â”€â”€ ğŸ“ file/          # File granularity data
â”‚
â””â”€â”€ ğŸ“ repos/                 # ğŸ“š Example repositories
    â””â”€â”€ ğŸ“ youtube-dl/        # Sample codebase for testing
```

## ğŸ”¥ Key Features

### ğŸ¨ Dual Granularity Processing

#### ğŸ“„ File-Level Granularity
- **ğŸ¯ Purpose**: High-level code understanding
- **âš¡ Process**: LLM-generated summaries for each file
- **ğŸ’¡ Benefits**: Contextual overview, reduced noise
- **ğŸ” Use Cases**: Architecture analysis, feature mapping
- **ğŸ’¾ Storage**: Summary + original content

#### ğŸ§© Chunk-Level Granularity
- **ğŸ¯ Purpose**: Detailed code analysis
- **âš¡ Process**: Intelligent text chunking
- **ğŸ’¡ Benefits**: Precise code sections, better accuracy
- **ğŸ” Use Cases**: Bug fixes, implementation details
- **ğŸ’¾ Storage**: Individual code chunks with metadata

### ğŸš€ Performance Features

#### âš¡ Smart Caching System
- **ğŸ“ Repository Caching**: Avoid reprocessing existing repos
- **ğŸ“ Summary Caching**: Cache LLM-generated summaries
- **ğŸ” Vector Store Persistence**: Save processed embeddings
- **ğŸ”„ Incremental Updates**: Process only new/modified files

#### ğŸ¯ Intelligent File Processing
- **ğŸŒ Multi-language Support**: Python, JS, Java, C++, etc.
- **ğŸš« Smart Filtering**: Exclude caches, logs, dependencies
- **ğŸ“Š Progress Tracking**: Real-time processing status
- **ğŸ›¡ï¸ Error Resilience**: Graceful handling of corrupted files

### ğŸ¤– AI Integration

#### ğŸ”· Azure OpenAI Integration
- **ğŸ¨ GPT Models**: Access to latest GPT models
- **ğŸ” Token Authentication**: Secure CloudGPT integration
- **âš™ï¸ Configurable Parameters**: Temperature, max tokens
- **ğŸŒ API Flexibility**: Multiple API versions support

#### ğŸ§  Embedding Models
- **ğŸ† High-Quality Embeddings**: Sentence Transformers
- **âš¡ GPU Acceleration**: CUDA support for faster processing
- **ğŸ”§ Model Flexibility**: Easy model switching
- **ğŸ’¾ Optimized Storage**: Efficient vector representations

## ğŸš€ Quick Start

### ğŸ“¦ Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd Code-Repository-RAG

# Install dependencies
pip install langchain langchain-community langchain-huggingface langchain-openai chromadb python-dotenv

# For Azure authentication (optional)
pip install azure-identity azure-identity-broker
```

### âš™ï¸ Configuration

Create a `.env` file in the project root:

```env
# ğŸ”· Azure OpenAI Configuration
AZURE_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_DEPLOYMENT=your-deployment-name
API_VERSION=your-api-version
```

### ğŸ® Basic Usage

```python
from main import CodeRepositoryRAG

# Initialize with file-level granularity
print("ğŸš€ Initializing Code Repository RAG System...")
rag_system = CodeRepositoryRAG(granularity="file")

# Process a repository
success = rag_system.run_full_pipeline("./repos/youtube-dl")

if success:
    print("âœ… System ready! Ask questions about the codebase.")

    # Ask intelligent questions
    questions = [
        "What is the main functionality of this project?",
        "How does the download mechanism work?",
        "What are the key classes and their purposes?"
    ]

    for question in questions:
        print(f"\nâ“ {question}")
        result = rag_system.ask_question(question)
        print(f"ğŸ’¡ {result['answer']}")
        print(f"ğŸ“š Sources: {len(result['context_sources'])} files")
```

### ğŸ¯ Advanced Usage

```python
# Initialize with chunk-level granularity for detailed analysis
rag_detailed = CodeRepositoryRAG(granularity="chunk")

# Custom configuration for specific needs
rag_detailed.chunk_size = 300  # Smaller chunks
rag_detailed.chunk_overlap = 50  # More overlap

# Process repository
rag_detailed.run_full_pipeline("./repos/youtube-dl")

# Interactive Q&A session
print("ğŸ¤– Interactive Code Analysis Session")
print("Type 'quit' to exit")

while True:
    question = input("\nâ“ Your question: ").strip()
    if question.lower() == 'quit':
        break

    result = rag_detailed.ask_question(question)

    print(f"\nğŸ’¡ Answer: {result['answer']}")
    print(f"ğŸ“š Referenced files: {', '.join(result['context_sources'])}")
    print(f"ğŸ“Š Total sources: {result['num_sources']}")
```

## ğŸ”§ Configuration Guide

### ğŸ›ï¸ System Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `granularity` | `"chunk"` | Processing mode: `"file"` or `"chunk"` |
| `chunk_size` | `500` | Text chunk size in characters |
| `chunk_overlap` | `200` | Overlap between chunks |
| `embedding_model` | `"all-MiniLM-L6-v2"` | HuggingFace embedding model |

### ğŸ”§ Advanced Configuration

```python
# Create custom RAG instance
rag = CodeRepositoryRAG()

# Modify processing parameters
rag.granularity = "file"  # or "chunk"
rag.chunk_size = 1000     # Larger chunks for better context
rag.chunk_overlap = 100   # Less overlap for efficiency

# Change embedding model
rag.embedding_model = "sentence-transformers/all-mpnet-base-v2"

# Custom persist directory
rag.processed_repos_dir = "./my_custom_cache"

# Initialize and use
rag.run_full_pipeline("./target-repo")
```

## ğŸ“Š Performance Analysis

### âš¡ Processing Performance

| Granularity | First Run | Cached Run | Memory Usage |
|-------------|-----------|------------|--------------|
| File-Level | ğŸŒ Slower (LLM calls) | âš¡ Fast (cached summaries) | ğŸ’š Lower |
| Chunk-Level | âš¡ Faster (no LLM) | âš¡ Fast (vector loading) | ğŸ’› Higher |

### ğŸ“ˆ Use Case Recommendations

#### Choose **File Granularity** when:
- ğŸ” Exploring new codebases
- ğŸ—ï¸ Understanding architecture
- ğŸ“– Getting high-level overviews
- ğŸ’¾ Working with smaller projects

#### Choose **Chunk Granularity** when:
- ğŸ› Debugging specific functions
- ğŸ”§ Analyzing implementation details
- ğŸ“š Working with large codebases
- ğŸ¯ Needing precise code sections

## ğŸ› ï¸ API Reference

### `CodeRepositoryRAG` Class

#### Constructor
```python
CodeRepositoryRAG(granularity="chunk")
```

**Parameters:**
- `granularity` (str): `"file"` or `"chunk"` processing mode

#### Core Methods

##### `run_full_pipeline(repo_path: str) -> bool`
ğŸš€ **Complete processing pipeline**
```python
success = rag.run_full_pipeline("./my-repo")
```

##### `ask_question(question: str) -> Dict[str, Any]`
â“ **Intelligent question answering**
```python
result = rag.ask_question("How does authentication work?")
print(result["answer"])  # The answer
print(result["context_sources"])  # Source files
print(result["num_sources"])  # Number of sources
```

##### `load_existing_vectorstore(repo_path: str) -> bool`
ğŸ’¾ **Load cached vector store**
```python
loaded = rag.load_existing_vectorstore("./my-repo")
```

##### `setup_qa_system()`
ğŸ¤– **Initialize Azure OpenAI integration**
```python
rag.setup_qa_system()
```

## ğŸ¨ Supported File Types

### âœ… Programming Languages
- ğŸ **Python**: `.py`
- ğŸŒ **JavaScript**: `.js`
- â˜• **Java**: `.java`
- âš¡ **C/C++**: `.c`, `.cpp`, `.h`
- ğŸ“„ **Documentation**: `.md`, `.txt`

### ğŸš« Excluded Patterns
- ğŸ“ `__pycache__/` - Python cache
- ğŸ“ `.git/` - Git repository
- ğŸ“ `node_modules/` - Dependencies
- ğŸ“„ `*.log` - Log files
- ğŸ“ Various build artifacts

## ğŸ’¾ Caching Strategy

### ğŸ—ï¸ Repository Hashing
- **ğŸ¯ Purpose**: Unique repository identification
- **ğŸ”§ Method**: MD5 hash of normalized path
- **ğŸ’¡ Benefits**: Multi-repository support

### ğŸ“ Summary Cache Structure
```
processed_repos/
â””â”€â”€ {repo_hash}/
    â””â”€â”€ {granularity}/
        â””â”€â”€ summary_cache/
            â”œâ”€â”€ src/main.py.txt
            â”œâ”€â”€ utils/helpers.py.txt
            â””â”€â”€ ...
```

### ğŸ” Vector Store Cache Structure
```
processed_repos/
â””â”€â”€ {repo_hash}/
    â””â”€â”€ {granularity}/
        â””â”€â”€ chroma_db/
            â”œâ”€â”€ chroma.sqlite3
            â””â”€â”€ {uuid}/
                â”œâ”€â”€ data_level0.bin
                â”œâ”€â”€ header.bin
                â””â”€â”€ ...
```

## ğŸ” Example Queries

### ğŸ“š Architecture Questions
```python
questions = [
    "What is the overall architecture of this system?",
    "What are the main components and their responsibilities?",
    "How do different modules interact with each other?",
    "What design patterns are used in this codebase?"
]
```

### ğŸ› Implementation Questions
```python
questions = [
    "How does the authentication system work?",
    "Explain the data flow in the main processing pipeline",
    "What are the key algorithms used in this project?",
    "How are errors handled throughout the system?"
]
```

### ğŸ”§ Code Quality Questions
```python
questions = [
    "What are the potential security vulnerabilities?",
    "Where can we improve error handling?",
    "What parts of the code need refactoring?",
    "Are there any performance bottlenecks?"
]
```

## ğŸ›¡ï¸ Error Handling

### ğŸ“ File Processing Errors
- **ğŸ”„ Graceful Skipping**: Continue with other files
- **ğŸ“Š Detailed Logging**: Track problematic files
- **ğŸ”„ Fallback Mechanisms**: Use raw content when parsing fails

### ğŸŒ Network/API Errors
- **ğŸ”„ Retry Logic**: Automatic retry for transient failures
- **ğŸ’¾ Cache Utilization**: Prevent repeated failed requests
- **ğŸ“Š Error Reporting**: Comprehensive error information

## âš¡ Optimization Tips

### ğŸš€ Performance Optimization
1. **ğŸ“Š Choose Right Granularity**: File-level for exploration, chunk-level for details
2. **ğŸ’¾ Leverage Caching**: Reuse processed repositories
3. **ğŸ”§ Adjust Chunk Size**: Balance context vs. precision
4. **âš¡ GPU Acceleration**: Use CUDA for embedding generation

### ğŸ’¾ Memory Optimization
1. **ğŸ—‚ï¸ Selective Processing**: Process only relevant files
2. **ğŸ§¹ Cache Cleanup**: Remove unused cached data
3. **ğŸ“¦ Chunk Size Tuning**: Smaller chunks for memory efficiency
4. **ğŸ”„ Incremental Updates**: Process only changes


## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **ğŸ¨ LangChain**: Core framework for LLM applications
- **ğŸ”· Azure OpenAI**: Powerful language models
- **ğŸ¤— HuggingFace**: High-quality embedding models
- **ğŸ’¾ Chroma**: Efficient vector database
- **ğŸ Python**: Amazing programming language

---

<div align="center">

**Made with â¤ï¸ for developers, by developers**

â­ **Star this repo** if you find it helpful!

[â¬†ï¸ Back to Top](#-code-repository-rag-system)

</div>